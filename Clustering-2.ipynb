{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e58369-7b2c-44b9-b7a7-d872b34e2f90",
   "metadata": {},
   "source": [
    "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n",
    "\n",
    "Answer(Q1):\n",
    "\n",
    "Hierarchical clustering is a clustering algorithm used in unsupervised machine learning to group data points into a hierarchy or tree-like structure of clusters. It differs from other clustering techniques, such as K-means or DBSCAN, in several key ways:\n",
    "\n",
    "1. **Hierarchy of Clusters**:\n",
    "   - **Hierarchical Clustering**: It organizes data points into a hierarchical structure, represented as a dendrogram. Each data point starts as its cluster and is successively merged with other clusters until a stopping criterion is met, resulting in a tree-like structure.\n",
    "   - **Other Clustering Techniques**: Other methods like K-means and DBSCAN do not create a hierarchical structure of clusters. They assign data points to a fixed number of clusters or determine clusters based on density.\n",
    "\n",
    "2. **No Need to Predefine the Number of Clusters**:\n",
    "   - **Hierarchical Clustering**: It does not require specifying the number of clusters (K) in advance. The hierarchy allows you to explore different levels of granularity by cutting the dendrogram at various heights.\n",
    "   - **Other Clustering Techniques**: Methods like K-means require you to specify the number of clusters before running the algorithm, which can be challenging.\n",
    "\n",
    "3. **Agglomerative or Divisive Approach**:\n",
    "   - **Hierarchical Clustering**: There are two main approaches in hierarchical clustering: agglomerative (bottom-up) and divisive (top-down). Agglomerative starts with individual data points as clusters and merges them, while divisive begins with all data points in one cluster and recursively divides them.\n",
    "   - **Other Clustering Techniques**: Most other techniques, including K-means and DBSCAN, follow a partition-based or density-based approach and do not build a hierarchy of clusters.\n",
    "\n",
    "4. **Cluster Similarity**:\n",
    "   - **Hierarchical Clustering**: It measures cluster similarity based on distance metrics or linkage criteria. Common linkage methods include single-linkage, complete-linkage, and average-linkage, which determine how the distance between clusters is calculated.\n",
    "   - **Other Clustering Techniques**: Clustering techniques like K-means use centroid-based or density-based approaches to determine cluster assignments.\n",
    "\n",
    "5. **Nested or Overlapping Clusters**:\n",
    "   - **Hierarchical Clustering**: It can reveal nested or overlapping clusters in the data. The hierarchy allows for the identification of both broad and fine-grained clusters.\n",
    "   - **Other Clustering Techniques**: Methods like K-means typically assign each data point to a single, non-overlapping cluster.\n",
    "\n",
    "6. **Visualization of Cluster Relationships**:\n",
    "   - **Hierarchical Clustering**: The dendrogram provides a visual representation of how clusters are related and allows for the exploration of different levels of granularity.\n",
    "   - **Other Clustering Techniques**: Visualization of cluster relationships is not as inherent in other techniques, which usually result in a single set of clusters without a hierarchical structure.\n",
    "\n",
    "7. **Computationally Intensive**:\n",
    "   - **Hierarchical Clustering**: It can be computationally intensive, especially when dealing with large datasets, as it requires calculating distances between all pairs of data points.\n",
    "   - **Other Clustering Techniques**: Some other techniques, like K-means, can be more computationally efficient, particularly with large datasets.\n",
    "\n",
    "8. **Robust to Initialization**:\n",
    "   - **Hierarchical Clustering**: It is less sensitive to the initial placement of data points compared to K-means, as it considers all data points in the hierarchy.\n",
    "   - **Other Clustering Techniques**: Methods like K-means can converge to different solutions depending on the initial cluster centroids.\n",
    "\n",
    "In summary, hierarchical clustering is a flexible and versatile clustering technique that provides a hierarchical representation of clusters, making it suitable for exploring data at different levels of granularity. Its agglomerative and divisive approaches, as well as the use of linkage criteria, make it distinct from other clustering methods like K-means and DBSCAN. Hierarchical clustering is particularly valuable when the number of clusters is not known in advance or when you want to analyze data in a hierarchical manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6e428-a2a8-40fc-8f95-3f6933b81202",
   "metadata": {},
   "source": [
    "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n",
    "\n",
    "\n",
    "Answer(Q2):\n",
    "\n",
    "Hierarchical clustering algorithms can be broadly categorized into two main types: agglomerative clustering and divisive clustering. Each type has a different approach to constructing the hierarchical structure of clusters:\n",
    "\n",
    "1. **Agglomerative Clustering**:\n",
    "   - **Approach**: Agglomerative clustering, also known as \"bottom-up\" clustering, starts with each data point as its own cluster and iteratively merges the most similar clusters until all data points belong to a single cluster or until a stopping criterion is met.\n",
    "   - **Process**:\n",
    "     1. Initially, each data point is treated as a separate cluster.\n",
    "     2. At each step, the two closest clusters, based on a specified distance metric (e.g., Euclidean distance or linkage criteria), are merged into a single cluster.\n",
    "     3. The process continues until all data points are part of a single cluster or until a predefined number of clusters or a specific linkage distance is reached.\n",
    "   - **Dendrogram**: Agglomerative clustering results in a hierarchical tree-like structure called a dendrogram, where each node represents a cluster, and the leaves are individual data points.\n",
    "\n",
    "2. **Divisive Clustering**:\n",
    "   - **Approach**: Divisive clustering, also known as \"top-down\" clustering, begins with all data points in a single cluster and recursively divides the cluster into smaller clusters until a stopping criterion is met.\n",
    "   - **Process**:\n",
    "     1. Initially, all data points belong to a single cluster.\n",
    "     2. At each step, the cluster is split into two or more smaller clusters, often using techniques like k-means or other clustering algorithms.\n",
    "     3. The process continues recursively, with each newly created cluster undergoing further division, until predefined criteria are satisfied (e.g., a specific number of clusters or a certain level of granularity).\n",
    "   - **Dendrogram**: Divisive clustering can also produce a dendrogram, but it is constructed from top to bottom, with each split resulting in a bifurcation of clusters.\n",
    "\n",
    "**Key Differences**:\n",
    "\n",
    "- Agglomerative clustering starts with individual data points as clusters and merges them into larger clusters, while divisive clustering starts with all data points in a single cluster and recursively divides them into smaller clusters.\n",
    "- Agglomerative clustering is more commonly used and discussed in the context of hierarchical clustering algorithms.\n",
    "- Divisive clustering is less common and tends to be less intuitive and computationally expensive than agglomerative clustering. It often requires specifying the number of clusters in advance, which can be a drawback.\n",
    "\n",
    "In practice, agglomerative clustering is the dominant form of hierarchical clustering because of its simplicity and flexibility. It allows data analysts to explore the hierarchical structure of the data without needing to predefine the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e632875-b7ad-4b3c-8317-48615869efeb",
   "metadata": {},
   "source": [
    "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?\n",
    "\n",
    "\n",
    "Answer(Q3):\n",
    "\n",
    "In hierarchical clustering, the distance between two clusters is a crucial concept because it determines which clusters are merged during the agglomeration process. Various distance metrics, also known as linkage criteria, are used to quantify the dissimilarity or similarity between clusters. Common distance metrics include:\n",
    "\n",
    "1. **Single Linkage (Minimum Linkage)**:\n",
    "   - **Definition**: The distance between two clusters is defined as the minimum distance between any pair of data points, one from each cluster.\n",
    "   - **Characteristics**: Single linkage tends to create clusters with elongated shapes and is sensitive to outliers or noise in the data.\n",
    "\n",
    "2. **Complete Linkage (Maximum Linkage)**:\n",
    "   - **Definition**: The distance between two clusters is defined as the maximum distance between any pair of data points, one from each cluster.\n",
    "   - **Characteristics**: Complete linkage tends to create compact and spherical clusters and is less sensitive to outliers compared to single linkage.\n",
    "\n",
    "3. **Average Linkage (UPGMA - Unweighted Pair Group Method with Arithmetic Mean)**:\n",
    "   - **Definition**: The distance between two clusters is defined as the average (mean) of all pairwise distances between data points, one from each cluster.\n",
    "   - **Characteristics**: Average linkage provides a balance between the extremes of single and complete linkage, often resulting in well-balanced clusters.\n",
    "\n",
    "4. **Centroid Linkage**:\n",
    "   - **Definition**: The distance between two clusters is defined as the distance between their centroids (the mean vectors of the data points in each cluster).\n",
    "   - **Characteristics**: Centroid linkage can produce clusters with varying shapes and sizes and is less affected by outliers than single linkage.\n",
    "\n",
    "5. **Ward's Linkage**:\n",
    "   - **Definition**: Ward's linkage minimizes the increase in the total within-cluster variance that results from merging two clusters. It uses the variance within clusters to assess the distance between clusters.\n",
    "   - **Characteristics**: Ward's linkage tends to produce compact and balanced clusters and is less sensitive to outliers.\n",
    "\n",
    "6. **Correlation-Based Linkage**:\n",
    "   - **Definition**: Correlation-based linkage uses correlation coefficients between clusters, with the idea that clusters with higher positive correlations are more similar.\n",
    "   - **Characteristics**: It is often used when dealing with data that has a strong correlation structure, such as gene expression data.\n",
    "\n",
    "7. **Other Custom Distance Metrics**:\n",
    "   - Depending on the nature of the data and the problem, custom distance metrics can be defined to suit specific requirements. These can include Mahalanobis distance, cosine similarity, Jaccard distance, or other domain-specific metrics.\n",
    "\n",
    "**Choosing the Right Linkage Criterion**:\n",
    "The choice of linkage criterion depends on the nature of the data and the problem you are trying to solve. There is no one-size-fits-all answer, and it may be necessary to try multiple linkage criteria and evaluate their impact on the clustering results. Some factors to consider when choosing a linkage criterion include the shape and size of clusters you expect, sensitivity to outliers, and the underlying structure of the data.\n",
    "\n",
    "Hierarchical clustering algorithms allow you to specify the linkage criterion as a parameter when performing the clustering, so you can experiment with different criteria to find the one that best suits your data and objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cefd5d-bae5-4618-a3af-c1521adef6ca",
   "metadata": {},
   "source": [
    "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?\n",
    "\n",
    "\n",
    "Answer(Q4):\n",
    "\n",
    "Determining the optimal number of clusters in hierarchical clustering can be a bit more challenging than in partition-based methods like K-means because hierarchical clustering produces a hierarchy of clusters. However, there are several methods and techniques you can use to decide the number of clusters or cut the dendrogram at an appropriate level. Here are some common approaches:\n",
    "\n",
    "1. **Visual Inspection of the Dendrogram**:\n",
    "   - **Method**: Plot the dendrogram generated by hierarchical clustering and visually inspect it. Look for a level of the dendrogram where the tree branches into a reasonable number of clusters, reflecting the desired granularity.\n",
    "   - **Interpretation**: Choose the number of clusters based on the visual assessment of the dendrogram.\n",
    "\n",
    "2. **Cutting the Dendrogram**:\n",
    "   - **Method**: Decide the number of clusters you want, and then cut the dendrogram at the corresponding height. The cut point can be chosen based on a specific distance threshold or by selecting a certain number of top levels.\n",
    "   - **Interpretation**: The clusters obtained after cutting the dendrogram represent the desired number of clusters.\n",
    "\n",
    "3. **Gap Statistics**:\n",
    "   - **Method**: Similar to K-means, you can calculate the gap statistic for different numbers of clusters in hierarchical clustering. This involves comparing the clustering quality of your data to that of a reference distribution.\n",
    "   - **Interpretation**: Choose the number of clusters where the gap statistic indicates a significant improvement over the reference distribution.\n",
    "\n",
    "4. **Dendrogram Inconsistency**:\n",
    "   - **Method**: Calculate the inconsistency coefficient for different levels in the dendrogram. The inconsistency coefficient measures the height of branches in the dendrogram relative to the average height at that level.\n",
    "   - **Interpretation**: Look for a level in the dendrogram where the inconsistency coefficient is high, indicating a potential natural division into clusters.\n",
    "\n",
    "5. **Silhouette Score**:\n",
    "   - **Method**: For each number of clusters, compute the silhouette score for the resulting clustering. The silhouette score measures the quality of clusters by assessing the separation between clusters and the cohesion within clusters.\n",
    "   - **Interpretation**: Choose the number of clusters that maximizes the silhouette score.\n",
    "\n",
    "6. **Cross-Validation**:\n",
    "   - **Method**: Split your data into training and validation sets and perform hierarchical clustering on the training set for different numbers of clusters. Evaluate the clustering quality on the validation set using an appropriate metric.\n",
    "   - **Interpretation**: Select the number of clusters that provides the best validation performance.\n",
    "\n",
    "7. **Expert Knowledge**:\n",
    "   - **Method**: Sometimes, domain knowledge or prior information about the data can help determine the optimal number of clusters. Consider consulting experts or using external criteria to guide your choice.\n",
    "\n",
    "8. **Hierarchical Cluster Evaluation Metrics**:\n",
    "   - **Method**: Some metrics, like cophenetic correlation and Davies-Bouldin index, can be used to evaluate the quality of hierarchical clustering for different numbers of clusters.\n",
    "   - **Interpretation**: Choose the number of clusters that yields the best values for these evaluation metrics.\n",
    "\n",
    "It's important to note that hierarchical clustering allows you to explore clusters at multiple levels of granularity, and the choice of the number of clusters may depend on the specific insights you seek from the data. Additionally, hierarchical clustering is less rigid than some other clustering methods, and the results can be adjusted by varying the height or level at which you cut the dendrogram. As such, it's often useful to use a combination of these methods and to explore different levels of clustering granularity to make an informed decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddd794-cb08-491b-b4de-bc44a48560a1",
   "metadata": {},
   "source": [
    "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n",
    "\n",
    "\n",
    "Answer(Q5):\n",
    "\n",
    "Dendrograms are graphical representations of the hierarchical structure of clusters created by hierarchical clustering algorithms. They are tree-like diagrams that illustrate the merging or splitting of clusters as the algorithm progresses. Dendrograms are a crucial tool for visualizing and interpreting the results of hierarchical clustering. Here's how dendrograms work and why they are useful in analyzing the results:\n",
    "\n",
    "**Structure of a Dendrogram**:\n",
    "- A dendrogram typically consists of a vertical axis (y-axis) representing the distance or dissimilarity between clusters or data points. The horizontal lines depict the individual data points or clusters.\n",
    "- The vertical lines, known as branches, connect the horizontal lines at heights that represent the similarity or distance between the clusters.\n",
    "- The longer the vertical line or branch, the greater the distance or dissimilarity between the clusters at that level of the hierarchy.\n",
    "\n",
    "**Dendrogram Construction Process**:\n",
    "- The construction of a dendrogram starts with each data point or individual cluster as a leaf node at the bottom of the tree.\n",
    "- As the hierarchical clustering algorithm proceeds, it iteratively merges or divides clusters based on the chosen linkage criterion and distance metric.\n",
    "- The dendrogram is built by connecting clusters (or individual data points) with branches as they are merged or split.\n",
    "\n",
    "**Usefulness of Dendrograms in Analyzing Results**:\n",
    "\n",
    "1. **Hierarchy Exploration**: Dendrograms allow you to explore the hierarchical structure of clusters at different levels of granularity. By examining the dendrogram, you can see how data points or clusters are grouped together at various heights, revealing patterns and subclusters.\n",
    "\n",
    "2. **Cluster Identification**: Dendrograms provide a visual representation of how clusters are formed during the hierarchical clustering process. You can easily identify the number of clusters and their relationships by looking at the branching structure.\n",
    "\n",
    "3. **Threshold Selection**: Dendrograms help in selecting a suitable threshold height or level at which to cut the tree to obtain the desired number of clusters. Cutting the dendrogram at different heights allows you to explore clusters of varying sizes and shapes.\n",
    "\n",
    "4. **Cluster Size and Composition**: The lengths of branches in the dendrogram indicate the relative sizes of the clusters. Longer branches represent larger clusters, while shorter branches indicate smaller clusters. By analyzing branch lengths, you can understand the composition of the clusters.\n",
    "\n",
    "5. **Outlier Detection**: Outliers often appear as isolated data points or clusters with very short branches in the dendrogram. Detecting these anomalies can be crucial in various applications.\n",
    "\n",
    "6. **Cluster Quality Assessment**: Dendrograms can provide insights into the quality of hierarchical clustering. Well-separated clusters with clear boundaries in the dendrogram suggest a good clustering solution, while messy or overlapping clusters may indicate issues.\n",
    "\n",
    "7. **Interpretability**: Dendrograms make the results of hierarchical clustering more interpretable and accessible to stakeholders who may not be familiar with the intricacies of the algorithm.\n",
    "\n",
    "8. **Hierarchical Relationships**: Dendrograms explicitly show the hierarchical relationships between clusters, revealing which clusters are merged early and which remain separate until later stages of clustering.\n",
    "\n",
    "In summary, dendrograms are essential tools for visualizing and interpreting the results of hierarchical clustering. They help you explore the hierarchical structure of clusters, identify the optimal number of clusters, and gain insights into the relationships and composition of clusters, making them valuable for data analysis and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c072c5b-ec2b-4d93-bc35-d8a7effe4533",
   "metadata": {},
   "source": [
    "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?\n",
    "\n",
    "\n",
    "Answer(Q6):\n",
    "\n",
    "Hierarchical clustering can be used for both numerical and categorical data, but the choice of distance metrics and linkage methods can differ based on the data type. Here's how hierarchical clustering can be adapted for each type of data:\n",
    "\n",
    "**1. Hierarchical Clustering for Numerical Data:**\n",
    "\n",
    "For numerical data, distance metrics that measure the dissimilarity or similarity between data points are commonly used. Common distance metrics include:\n",
    "\n",
    "- **Euclidean Distance**: The most widely used metric for numerical data. It calculates the straight-line distance between two data points in a multidimensional space.\n",
    "\n",
    "- **Manhattan Distance (City Block Distance)**: Measures the sum of the absolute differences between corresponding coordinates of two data points. It's often used when data lie along axes at right angles.\n",
    "\n",
    "- **Minkowski Distance**: A generalization of both Euclidean and Manhattan distances. It allows you to adjust the sensitivity to different features through a parameter (e.g., p = 2 for Euclidean, p = 1 for Manhattan).\n",
    "\n",
    "- **Correlation Distance**: Measures the correlation between two data points, with values ranging from -1 (perfect negative correlation) to 1 (perfect positive correlation). It's suitable for data with varying scales or units.\n",
    "\n",
    "- **Cosine Distance**: Measures the cosine of the angle between two data vectors. It's often used for text analysis and high-dimensional data where the magnitude of vectors is important.\n",
    "\n",
    "- **Mahalanobis Distance**: Takes into account the covariance structure of the data, making it suitable for data with correlated features.\n",
    "\n",
    "**2. Hierarchical Clustering for Categorical Data:**\n",
    "\n",
    "For categorical data, distance metrics need to be adapted to measure dissimilarity between categories. Commonly used distance metrics for categorical data include:\n",
    "\n",
    "- **Jaccard Distance**: Measures the dissimilarity between two sets, such as the presence or absence of categorical variables. It's suitable for binary data or nominal categorical data.\n",
    "\n",
    "- **Hamming Distance**: Counts the number of positions at which two categorical vectors differ. It's used for categorical data with a fixed number of categories or binary data.\n",
    "\n",
    "- **Dice Distance**: Similar to Jaccard distance but often used when there is a lot of overlap between categories. It measures the relative overlap between two sets.\n",
    "\n",
    "- **Categorical Distance**: A custom distance metric that considers the co-occurrence patterns of categories. It can be tailored to the specific characteristics of the categorical data.\n",
    "\n",
    "**Handling Mixed Data (Numerical and Categorical):**\n",
    "\n",
    "In cases where you have both numerical and categorical data, you can employ hybrid distance metrics or feature engineering techniques to handle the mixed data types. For example:\n",
    "\n",
    "- **Gower's Distance**: Gower's distance is a generalized distance metric that can handle mixed data types, including numerical, ordinal, and categorical data. It computes the weighted average of different distances depending on the data type.\n",
    "\n",
    "- **Feature Transformation**: You can convert categorical data into numerical form using techniques like one-hot encoding, binary encoding, or ordinal encoding, and then use standard numerical distance metrics for the combined data.\n",
    "\n",
    "- **Custom Metrics**: Design custom distance metrics that account for the characteristics of your mixed data.\n",
    "\n",
    "It's important to choose appropriate distance metrics and linkage methods based on the nature of your data, as using inappropriate metrics can lead to suboptimal clustering results. Additionally, consider preprocessing steps like feature scaling or encoding to ensure that all features contribute effectively to the clustering process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0f9d2-2d69-42ee-bb42-b5d365c36129",
   "metadata": {},
   "source": [
    "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?\n",
    "\n",
    "Answer(Q7):\n",
    "\n",
    "Hierarchical clustering can be used to identify outliers or anomalies in your data by leveraging the dendrogram structure and the concept of dissimilarity or distance between data points. Here's a step-by-step approach to using hierarchical clustering for outlier detection:\n",
    "\n",
    "1. **Perform Hierarchical Clustering**:\n",
    "   - Apply hierarchical clustering to your dataset using an appropriate distance metric and linkage method.\n",
    "   - Create a dendrogram that illustrates the hierarchical structure of clusters.\n",
    "\n",
    "2. **Visual Inspection of the Dendrogram**:\n",
    "   - Examine the dendrogram visually to identify clusters that appear to be significantly dissimilar from others or isolated from the main branches.\n",
    "   - Outliers may be represented by clusters with very short branches or individual data points that branch off early in the hierarchy.\n",
    "\n",
    "3. **Determine a Height Threshold**:\n",
    "   - Select a height or dissimilarity threshold in the dendrogram that defines the level beyond which clusters are considered outliers.\n",
    "   - The choice of the threshold depends on your domain knowledge and the specific characteristics of your data.\n",
    "\n",
    "4. **Identify Outliers**:\n",
    "   - Use the chosen threshold to cut the dendrogram, separating clusters into those considered normal or inliers and those considered outliers.\n",
    "   - Data points in clusters below the threshold are considered inliers, while those in clusters above the threshold are identified as outliers.\n",
    "\n",
    "5. **Label and Analyze Outliers**:\n",
    "   - Label the identified outliers in your dataset.\n",
    "   - Analyze the outliers to gain insights into why they are distinct from the rest of the data. This analysis may involve examining their feature values, patterns, or characteristics that make them outliers.\n",
    "\n",
    "6. **Consider Multiple Thresholds**:\n",
    "   - Depending on the problem, you can consider using multiple thresholds to identify different levels of outliers or anomalies. Some outliers may be more extreme than others.\n",
    "\n",
    "7. **Validation and Further Analysis**:\n",
    "   - Validate the identified outliers using appropriate techniques, such as cross-validation or external validation if ground-truth labels are available.\n",
    "   - Further analyze the outliers to understand their impact on the dataset and assess whether they should be treated as genuine anomalies or erroneous data.\n",
    "\n",
    "It's important to note that hierarchical clustering for outlier detection is a semi-supervised approach that relies on visual inspection and the choice of a threshold. The effectiveness of this method depends on the choice of distance metric, linkage method, and the threshold used to identify outliers. It may work well for datasets with clear hierarchical structures, but it may be less effective for high-dimensional or noisy data. Therefore, it's advisable to combine hierarchical clustering with other outlier detection methods and validation techniques for robust results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
