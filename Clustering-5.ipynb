{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ec6cfa-8330-4ba4-8b7a-0c37a5f44cd7",
   "metadata": {},
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "\n",
    "\n",
    "Answer(Q1):\n",
    "\n",
    "A contingency matrix, also known as a confusion matrix, is a table used in the field of machine learning and statistics to evaluate the performance of a classification model, especially in binary classification tasks. It is a square matrix that compares the predicted classification results of a model with the actual ground truth values. The matrix is typically organized as follows:\n",
    "\n",
    "                  Actual Class 0     Actual Class 1\n",
    "              \n",
    "Predicted Class 0------True Negative (TN)------False Negative (FN)\n",
    "\n",
    "Predicted Class 1------False Positive (FP)------True Positive (TP)\n",
    "\n",
    "\n",
    "Here's what each of these terms means:\n",
    "\n",
    "1. True Positive (TP): This represents the number of instances correctly predicted as the positive class by the model. These are the cases where the model correctly identified positive outcomes.\n",
    "\n",
    "2. True Negative (TN): This represents the number of instances correctly predicted as the negative class by the model. These are the cases where the model correctly identified negative outcomes.\n",
    "\n",
    "3. False Positive (FP): This represents the number of instances incorrectly predicted as the positive class by the model when they are actually negative. These are also known as Type I errors or false alarms.\n",
    "\n",
    "4. False Negative (FN): This represents the number of instances incorrectly predicted as the negative class by the model when they are actually positive. These are also known as Type II errors or misses.\n",
    "\n",
    "By analyzing the values in the contingency matrix, you can compute various performance metrics for your classification model, including:\n",
    "\n",
    "1. **Accuracy**: It measures the overall correctness of the model's predictions and is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "\n",
    "2. **Precision (Positive Predictive Value)**: It measures the accuracy of positive predictions and is calculated as TP / (TP + FP).\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate)**: It measures the model's ability to identify all positive instances and is calculated as TP / (TP + FN).\n",
    "\n",
    "4. **Specificity (True Negative Rate)**: It measures the model's ability to identify all negative instances and is calculated as TN / (TN + FP).\n",
    "\n",
    "5. **F1 Score**: It is the harmonic mean of precision and recall and provides a balance between these two metrics. It is calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "6. **False Positive Rate (FPR)**: It measures the proportion of actual negative instances that are incorrectly predicted as positive and is calculated as FP / (FP + TN).\n",
    "\n",
    "These metrics help you assess the quality and effectiveness of your classification model. Depending on the specific problem and requirements, you may prioritize different metrics. For example, in medical diagnosis, recall (to minimize false negatives) may be more critical, while in spam email classification, precision (to minimize false positives) may be more important. The choice of the appropriate metric depends on the specific goals and constraints of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904d03e-5194-45cb-b3b2-307762cda72b",
   "metadata": {},
   "source": [
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?\n",
    "\n",
    "\n",
    "\n",
    "Answer(Q2):\n",
    "\n",
    "A pair confusion matrix, also known as a confusion matrix for pairwise classification or a one-vs-one confusion matrix, is used in situations where you are dealing with a multi-class classification problem and want to evaluate the performance of your classifier in a pairwise manner. It is different from a regular confusion matrix, which is typically used for binary classification or when you are interested in overall class-level performance metrics in a multi-class setting.\n",
    "\n",
    "Here's how a pair confusion matrix differs from a regular confusion matrix:\n",
    "\n",
    "**Regular Confusion Matrix (Multi-Class)**:\n",
    "In a regular confusion matrix for multi-class classification, you have rows and columns representing the true and predicted class labels for all the individual classes. The matrix helps you understand how well your classifier performs for each individual class. Each cell in the matrix represents the count or proportion of instances that were classified into a specific true class and a specific predicted class.\n",
    "\n",
    "For example, in a multi-class problem with three classes (Class A, Class B, and Class C), the regular confusion matrix might look like this:\n",
    "\n",
    "\n",
    "            Predicted Class A   Predicted Class B   Predicted Class C\n",
    "                 \n",
    "Actual Class A-----------50-----------------------------5 -----------------------------2\n",
    "\n",
    "Actual Class B-----------3-----------------------------45------------------------------6\n",
    "\n",
    "Actual Class C-----------1------------------------------4------------------------------60\n",
    "\n",
    "\n",
    "\n",
    "**Pair Confusion Matrix (Pairwise Classification)**:\n",
    "In a pair confusion matrix, you are interested in evaluating the performance of your classifier in a one-vs-one or pairwise fashion. This means that you create a confusion matrix for each pair of classes, comparing how well your classifier distinguishes between those two specific classes, ignoring the others. For a multi-class problem with three classes, you would create three pair confusion matrices: one for (Class A vs. Class B), one for (Class A vs. Class C), and one for (Class B vs. Class C).\n",
    "\n",
    "For example, in the pair confusion matrix for (Class A vs. Class B), you might have:\n",
    "\n",
    "\n",
    "              Predicted Class A   Predicted Class B\n",
    "                 \n",
    "Actual Class A-----------50------------------------5\n",
    "\n",
    "Actual Class B-----------3-------------------------45\n",
    "\n",
    "\n",
    "\n",
    "Pair confusion matrices can be useful in certain situations for the following reasons:\n",
    "\n",
    "1. **Fine-Grained Analysis**: Pairwise evaluation allows you to gain a more detailed understanding of how well your classifier discriminates between specific classes. This can be particularly valuable when certain class distinctions are more critical or challenging.\n",
    "\n",
    "2. **Imbalanced Data**: In situations where your classes have imbalanced distributions, it can be more informative to evaluate the classifier's performance for each pair separately, as imbalances can affect overall metrics.\n",
    "\n",
    "3. **Error Analysis**: Pairwise confusion matrices can help you identify which specific class pairs are causing the most errors or misclassifications, allowing you to focus on improving the classifier's performance in those areas.\n",
    "\n",
    "4. **Multi-Class Extensions**: Pairwise evaluation can be extended to multi-class problems, where it might be impractical to create a confusion matrix for all classes due to space and readability constraints.\n",
    "\n",
    "However, it's important to note that while pair confusion matrices provide valuable insights into pairwise class distinctions, they do not provide a complete picture of the classifier's overall performance across all classes. Therefore, they are often used in conjunction with regular confusion matrices and other evaluation metrics to assess the classifier comprehensively in a multi-class setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906594a1-8435-4d68-9689-aa32c5da895f",
   "metadata": {},
   "source": [
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?\n",
    "\n",
    "\n",
    "Answer(Q3):\n",
    "\n",
    "In the context of natural language processing (NLP) and language models, an extrinsic measure, also known as an application-level or downstream evaluation, assesses the performance of a language model based on its effectiveness in solving a specific real-world task or application. This is in contrast to intrinsic measures, which evaluate a model's performance based on its internal properties or capabilities, such as fluency or language modeling perplexity.\n",
    "\n",
    "Extrinsic measures are typically used to evaluate the practical utility and effectiveness of language models in applications or tasks that involve natural language understanding and generation. Here's how extrinsic measures work and how they are typically used:\n",
    "\n",
    "1. **Selecting a Real-World Task**: Researchers or developers choose a specific NLP task or application that they are interested in evaluating the language model on. This could be a wide range of tasks, including machine translation, text summarization, sentiment analysis, question answering, language generation, and more.\n",
    "\n",
    "2. **Integration of the Language Model**: The language model is integrated into the pipeline or system designed for the chosen task. This integration can vary depending on the task. For example, for machine translation, the model might be used to generate translations, while for question answering, it might be used to generate answers to user queries.\n",
    "\n",
    "3. **Evaluation on Task Performance**: The language model is then evaluated based on how well it performs on the chosen task. The evaluation typically involves using established metrics specific to that task. For example:\n",
    "   - For machine translation, the evaluation might use metrics like BLEU, METEOR, or ROUGE.\n",
    "   - For sentiment analysis, accuracy, F1-score, or area under the ROC curve (AUC) might be used.\n",
    "   - For question answering, metrics like precision, recall, F1-score, or accuracy on correct answers may be used.\n",
    "\n",
    "4. **Benchmarking and Comparison**: Language models are often benchmarked against other models or baselines to assess their relative performance on the chosen task. This helps in understanding whether the language model provides improvements in real-world applications.\n",
    "\n",
    "5. **Iterative Development**: The results of the extrinsic evaluation can guide iterative development and fine-tuning of the language model to improve its performance on the specific task. This might involve techniques like transfer learning or domain adaptation to better suit the task.\n",
    "\n",
    "Extrinsic measures are crucial because they provide a practical assessment of how well a language model can be applied to solve real-world problems. While intrinsic measures, like perplexity or language modeling accuracy, offer insights into the language model's proficiency, they don't necessarily reflect its performance in real applications. Extrinsic measures bridge the gap between model capabilities and practical utility, making them a key component of NLP model evaluation. Researchers and developers often rely on a combination of intrinsic and extrinsic measures to comprehensively evaluate language models in NLP research and development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abfed8-d08b-45f4-8924-925c5bd4d313",
   "metadata": {},
   "source": [
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?\n",
    "\n",
    "\n",
    "Answer(Q4):\n",
    "\n",
    "In the context of machine learning, intrinsic measures and extrinsic measures are two different types of evaluation metrics used to assess the performance and capabilities of models, and they serve distinct purposes:\n",
    "\n",
    "1. **Intrinsic Measures**:\n",
    "\n",
    "   - **Definition**: Intrinsic measures, also known as internal or model-level evaluation metrics, assess a model's performance based on its inherent characteristics or capabilities. These metrics evaluate how well a model can perform specific tasks or aspects directly related to its architecture, without considering its performance in real-world applications.\n",
    "\n",
    "   - **Examples**: Intrinsic measures can include metrics like accuracy, precision, recall, F1-score, perplexity, cross-entropy loss, and other task-specific evaluation metrics. For instance, in a language model, perplexity measures how well the model predicts the next word in a sequence based on the underlying language modeling capabilities.\n",
    "\n",
    "   - **Use Cases**: Intrinsic measures are typically used during model development and fine-tuning. They help researchers and developers understand the model's behavior, capabilities, and weaknesses. For example, in natural language processing, language models are often evaluated based on perplexity or accuracy on a language modeling task to gauge their language understanding and generation abilities.\n",
    "\n",
    "2. **Extrinsic Measures**:\n",
    "\n",
    "   - **Definition**: Extrinsic measures, also known as application-level or downstream evaluation metrics, assess a model's performance in solving specific real-world tasks or applications. These metrics evaluate how effectively the model can be applied to practical use cases and address the primary objectives of those use cases.\n",
    "\n",
    "   - **Examples**: Extrinsic measures depend on the specific application or task. For instance, in a machine translation application, extrinsic measures may include BLEU (Bilingual Evaluation Understudy) score or METEOR score, which assess the quality of translated text. In a sentiment analysis application, accuracy, F1-score, or AUC (Area Under the ROC Curve) may be used to evaluate the model's performance in sentiment classification.\n",
    "\n",
    "   - **Use Cases**: Extrinsic measures are used to assess a model's practical utility. They are typically employed in real-world applications to determine whether a model can effectively solve specific tasks or problems. For example, in the development of a chatbot, extrinsic measures might evaluate its ability to provide accurate and contextually relevant responses to user queries.\n",
    "\n",
    "**Key Differences**:\n",
    "\n",
    "- Intrinsic measures evaluate a model's performance based on its inherent capabilities and are often used for model development and analysis.\n",
    "\n",
    "- Extrinsic measures evaluate a model's performance in real-world applications and focus on how well the model addresses specific tasks or objectives.\n",
    "\n",
    "- Intrinsic measures are typically task-agnostic and assess general model properties, while extrinsic measures are task-specific and assess the model's performance in particular applications.\n",
    "\n",
    "In practice, both intrinsic and extrinsic measures play complementary roles in model evaluation. Intrinsic measures help researchers and developers understand the model's fundamental strengths and weaknesses, while extrinsic measures determine whether a model can effectively serve its intended real-world purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ecf9d-f266-4919-a72e-ce314376a2ea",
   "metadata": {},
   "source": [
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?\n",
    "\n",
    "\n",
    "Answer(Q5):\n",
    "\n",
    "A confusion matrix is a fundamental tool in machine learning used to evaluate the performance of classification models, especially in binary and multi-class classification problems. Its primary purpose is to provide a detailed breakdown of a model's predictions compared to the actual ground truth labels. It helps in understanding how well the model is performing, identifying areas of strength, and pinpointing weaknesses. Here's how it works:\n",
    "\n",
    "**Purpose of a Confusion Matrix**:\n",
    "\n",
    "The primary purposes of a confusion matrix are as follows:\n",
    "\n",
    "1. **Performance Evaluation**: A confusion matrix provides a comprehensive summary of a classifier's predictions, showing which instances were classified correctly and which were misclassified. This information is essential for assessing the model's overall performance.\n",
    "\n",
    "2. **Identification of Errors**: It helps identify the types of errors made by the classifier, such as false positives and false negatives. This is crucial for understanding where the model is failing and why.\n",
    "\n",
    "3. **Evaluation Metrics**: The confusion matrix serves as the basis for calculating various evaluation metrics, including accuracy, precision, recall, F1-score, specificity, and more. These metrics provide quantitative measures of the model's performance.\n",
    "\n",
    "4. **Model Comparison**: You can use confusion matrices to compare the performance of different models or algorithms on the same dataset, helping you determine which one is better suited for the task.\n",
    "\n",
    "**Components of a Confusion Matrix**:\n",
    "\n",
    "In a binary classification confusion matrix, there are four main components:\n",
    "\n",
    "- **True Positives (TP)**: Instances that were correctly classified as positive by the model.\n",
    "\n",
    "- **True Negatives (TN)**: Instances that were correctly classified as negative by the model.\n",
    "\n",
    "- **False Positives (FP)**: Instances that were incorrectly classified as positive when they are actually negative (Type I errors).\n",
    "\n",
    "- **False Negatives (FN)**: Instances that were incorrectly classified as negative when they are actually positive (Type II errors).\n",
    "\n",
    "**Using a Confusion Matrix to Identify Strengths and Weaknesses**:\n",
    "\n",
    "1. **Overall Model Performance**: The confusion matrix provides a quick overview of how many instances your model got right (TP and TN) and how many it got wrong (FP and FN). This helps you assess the model's overall performance.\n",
    "\n",
    "2. **Precision and Recall Analysis**: By examining the values in the confusion matrix, you can calculate precision (TP / (TP + FP)) and recall (TP / (TP + FN)). Precision tells you how well the model avoids false positives, while recall tells you how well it captures all positive instances. Balancing these metrics can help you identify areas of improvement.\n",
    "\n",
    "3. **F1-Score Analysis**: The F1-score, which is the harmonic mean of precision and recall, provides a balanced measure of a model's performance. It helps identify models that perform well on both false positives and false negatives.\n",
    "\n",
    "4. **Class-Specific Analysis**: In multi-class problems, confusion matrices can provide insights into which classes the model struggles with the most. You can identify whether the model consistently misclassifies certain classes or if the errors are more evenly distributed across classes.\n",
    "\n",
    "5. **Threshold Adjustment**: Depending on the application, you might adjust the classification threshold to prioritize precision or recall. Analyzing the confusion matrix can help you make an informed decision about threshold tuning.\n",
    "\n",
    "In summary, a confusion matrix is a valuable tool for assessing and diagnosing the performance of classification models. It provides insights into the strengths and weaknesses of a model, guides further model improvements, and aids in decision-making for model deployment and fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c47544-99b0-4898-be16-c3685e1d1546",
   "metadata": {},
   "source": [
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?\n",
    "\n",
    "\n",
    "Answer(Q6):\n",
    "\n",
    "In the context of unsupervised learning, intrinsic measures are used to evaluate the performance of algorithms that do not rely on labeled data for training. These measures assess various properties of the clustering or dimensionality reduction results produced by unsupervised learning algorithms. Here are some common intrinsic measures and how they can be interpreted:\n",
    "\n",
    "1. **Silhouette Score**:\n",
    "   - **Interpretation**: The silhouette score measures how similar each data point in one cluster is to the other data points in the same cluster compared to the nearest neighboring cluster. It ranges from -1 to +1. A high silhouette score indicates that the data points within clusters are close to each other, and they are well separated from other clusters. A low or negative score suggests that data points may be in the wrong cluster or that clusters are overlapping.\n",
    "   - **Use Cases**: Silhouette score is commonly used for evaluating clustering algorithms, such as K-Means.\n",
    "\n",
    "2. **Davies-Bouldin Index**:\n",
    "   - **Interpretation**: The Davies-Bouldin index measures the average similarity between each cluster and its most similar cluster. A lower Davies-Bouldin index indicates better clustering, with well-separated and distinct clusters. Conversely, a higher index suggests that clusters are either too close or overlapping.\n",
    "   - **Use Cases**: This index is also used for evaluating clustering algorithms.\n",
    "\n",
    "3. **Dunn Index**:\n",
    "   - **Interpretation**: The Dunn index quantifies the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance. A higher Dunn index implies better clustering, as it indicates that clusters are compact (small intra-cluster distances) and well-separated (large inter-cluster distances).\n",
    "   - **Use Cases**: The Dunn index is used to assess the quality of clusters generated by clustering algorithms.\n",
    "\n",
    "4. **Inertia (Within-Cluster Sum of Squares)**:\n",
    "   - **Interpretation**: Inertia measures the total distance between data points and their cluster's centroid. Lower inertia values suggest that data points within clusters are closer to their centroids, indicating better clustering.\n",
    "   - **Use Cases**: Inertia is often used to determine the optimal number of clusters in K-Means clustering by plotting the inertia for different values of K (elbow method).\n",
    "\n",
    "5. **Explained Variance Ratio (PCA)**:\n",
    "   - **Interpretation**: In dimensionality reduction tasks using Principal Component Analysis (PCA), the explained variance ratio measures the proportion of the total variance in the data explained by each principal component. A higher explained variance ratio indicates that the principal components retain more information, leading to better dimensionality reduction.\n",
    "   - **Use Cases**: This measure is relevant when performing PCA for dimensionality reduction.\n",
    "\n",
    "6. **Gap Statistics**:\n",
    "   - **Interpretation**: Gap statistics compare the performance of a clustering algorithm to that of a random clustering assignment. A larger gap between the clustering algorithm's performance and random assignment indicates the algorithm's effectiveness.\n",
    "   - **Use Cases**: Gap statistics help assess the significance of clusters and can be used for choosing the optimal number of clusters.\n",
    "\n",
    "7. **Adjusted Rand Index (ARI)**:\n",
    "   - **Interpretation**: ARI quantifies the similarity between the true labels and the labels produced by a clustering algorithm, correcting for chance. An ARI score of 1 indicates perfect clustering, while a score close to 0 suggests random clustering.\n",
    "   - **Use Cases**: ARI is a measure of clustering quality and is used for evaluating the accuracy of cluster assignments.\n",
    "\n",
    "When interpreting these intrinsic measures, it's important to keep in mind that the choice of the appropriate measure depends on the specific unsupervised learning task (e.g., clustering or dimensionality reduction) and the characteristics of the data. In practice, it's often advisable to use multiple measures to get a comprehensive assessment of the algorithm's performance. Additionally, the interpretation of these measures should be considered alongside domain knowledge and the specific objectives of the unsupervised learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ae9ba-e8eb-4ca6-b94f-c0eb853a45f3",
   "metadata": {},
   "source": [
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?\n",
    "\n",
    "Answer(Q7):\n",
    "\n",
    "Using accuracy as the sole evaluation metric for classification tasks has certain limitations that can provide an incomplete or even misleading picture of a model's performance. Here are some of the key limitations of accuracy and strategies to address them:\n",
    "\n",
    "1. **Imbalanced Datasets**:\n",
    "   - **Limitation**: Accuracy can be misleading when dealing with imbalanced datasets, where one class significantly outnumbers the others. A classifier that predicts the majority class for all instances can achieve high accuracy while being practically useless.\n",
    "   - **Addressing**: Consider using alternative metrics like precision, recall, F1-score, or area under the ROC curve (AUC-ROC) that provide a more balanced assessment of model performance, especially for minority classes. Resampling techniques (e.g., oversampling, undersampling) and synthetic data generation can also help mitigate class imbalance issues.\n",
    "\n",
    "2. **Misclassification Costs**:\n",
    "   - **Limitation**: Accuracy treats all misclassifications equally, but in many real-world scenarios, misclassifying certain classes can have higher costs or implications than others.\n",
    "   - **Addressing**: Use cost-sensitive learning approaches where you assign different misclassification costs to different classes. This can involve adjusting the decision threshold, using class-specific metrics, or incorporating cost matrices into the modeling process.\n",
    "\n",
    "3. **Multiclass Problems**:\n",
    "   - **Limitation**: Accuracy can be problematic for multiclass classification, as it doesn't provide insights into which specific classes the model struggles with.\n",
    "   - **Addressing**: Employ metrics like confusion matrices, class-specific precision and recall, and micro-average or macro-average versions of precision, recall, and F1-score to assess performance across all classes. These metrics offer a more detailed view of the model's behavior.\n",
    "\n",
    "4. **Ambiguity in Class Definitions**:\n",
    "   - **Limitation**: In some cases, class definitions may be ambiguous, and there may be uncertainty in the ground truth labels. Accuracy assumes that each instance has a definitive and correct label.\n",
    "   - **Addressing**: Consider using probabilistic classification or models that provide confidence scores for predictions. You can then use thresholds to make decisions based on confidence levels or explore Bayesian approaches that incorporate uncertainty.\n",
    "\n",
    "5. **Sequential and Temporal Data**:\n",
    "   - **Limitation**: In tasks involving sequential or temporal data, such as time series classification, the timing of predictions can be important, and accuracy alone may not capture this temporal aspect.\n",
    "   - **Addressing**: Use time-based metrics like precision-recall curves over time or consider incorporating time-sensitive objectives and evaluation criteria, such as the area under the precision-recall curve (AUC-PR) for time series data.\n",
    "\n",
    "6. **Model Bias and Fairness**:\n",
    "   - **Limitation**: Accuracy can mask bias in models, where certain demographic groups are consistently misclassified more than others. This can lead to unfair or discriminatory outcomes.\n",
    "   - **Addressing**: Evaluate model fairness using fairness metrics like disparate impact, equal opportunity difference, or equalized odds. Also, consider using techniques like re-sampling, re-weighting, or adversarial debiasing to mitigate bias in model predictions.\n",
    "\n",
    "7. **Changing Data Distributions**:\n",
    "   - **Limitation**: Accuracy assumes that the data distribution remains stable over time. If the distribution changes, the accuracy of a model may not accurately reflect its current performance.\n",
    "   - **Addressing**: Implement strategies like monitoring model performance over time, using drift detection techniques to detect changes in data distribution, and retraining models when necessary to adapt to changing conditions.\n",
    "\n",
    "In summary, while accuracy is a valuable metric for classification tasks, it should be used in conjunction with other relevant metrics to provide a more comprehensive and context-specific assessment of model performance. The choice of metrics should align with the specific objectives, challenges, and characteristics of the classification problem at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
